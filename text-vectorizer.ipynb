{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excited-documentation",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-09T20:37:11.293930Z",
     "iopub.status.busy": "2021-05-09T20:37:11.292213Z",
     "iopub.status.idle": "2021-05-09T20:37:18.492060Z",
     "shell.execute_reply": "2021-05-09T20:37:18.491354Z"
    },
    "papermill": {
     "duration": 7.210108,
     "end_time": "2021-05-09T20:37:18.492225",
     "exception": false,
     "start_time": "2021-05-09T20:37:11.282117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn .model_selection import StratifiedKFold, GroupKFold\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "internal-projector",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-09T20:37:18.508073Z",
     "iopub.status.busy": "2021-05-09T20:37:18.507419Z",
     "iopub.status.idle": "2021-05-09T20:37:18.715633Z",
     "shell.execute_reply": "2021-05-09T20:37:18.716084Z"
    },
    "papermill": {
     "duration": 0.218517,
     "end_time": "2021-05-09T20:37:18.716247",
     "exception": false,
     "start_time": "2021-05-09T20:37:18.497730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \n",
       "0                          Paper Bag Victoria Secret            0  \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...            1  \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr            2  \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...            3  \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml            4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "df['label_group'], unique = df['label_group'].factorize()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lyric-spencer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-09T20:37:18.735797Z",
     "iopub.status.busy": "2021-05-09T20:37:18.730299Z",
     "iopub.status.idle": "2021-05-09T20:37:18.760486Z",
     "shell.execute_reply": "2021-05-09T20:37:18.759940Z"
    },
    "papermill": {
     "duration": 0.038509,
     "end_time": "2021-05-09T20:37:18.760633",
     "exception": false,
     "start_time": "2021-05-09T20:37:18.722124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing function helper\n",
    "# replace word that concatenate with other word\n",
    "def remove_concatenate_2_words(text):\n",
    "    list_words = ['khusus']\n",
    "    for w in list_words:\n",
    "        text = text.replace(w, '')\n",
    "    return text\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "STOPWORDS_ID = set(stopwords.words('english'))\n",
    "STOPWORDS_EN = set(stopwords.words('indonesian'))\n",
    "def remove_stopwords(list_text):\n",
    "    text_not_in_ID = [word for word in list_text if word not in STOPWORDS_EN]\n",
    "    text = [word for word in text_not_in_ID if word not in STOPWORDS_ID]\n",
    "    return text\n",
    "\n",
    "# remove big number and split text that contains word and number\n",
    "def remove_big_number(list_text):\n",
    "    words = []\n",
    "    for w in list_text:\n",
    "        sub_w = re.split('(\\d+)',w)\n",
    "        for item in sub_w:\n",
    "            try:\n",
    "                tmp = int(item)\n",
    "                if tmp < 7000:\n",
    "                    if (tmp>1000) and (tmp % 100 == 0): # for even number\n",
    "                        words.append(str(tmp))\n",
    "                    elif (tmp<=1000) and (tmp>100) and (tmp % 10 == 0 ):\n",
    "                        words.append(str(tmp))\n",
    "                    elif (tmp<=100) and (tmp % 2 == 0):\n",
    "                        words.append(str(tmp))\n",
    "            except:\n",
    "                words.append(item)\n",
    "    return words\n",
    "\n",
    "def remove_zero_val(list_text):\n",
    "    return [w for w in list_text if w not in ['0']]\n",
    "\n",
    "def remove_common_words(list_text):\n",
    "    common_words = \"hari keren kere kw super baik jual jualan quality best free  kwalitas berkualitas kualitas bagus terbaik kembali dijamin beli gratis murah free diskon ongkir cek berkualitas original asli kualitas uang jaminan jamin terjamin buatan buat kirim wilayah luar kota jawa bali jakarta surabaya bulan month year day tahun hari harian anda your nikmat singapore malaysia indonesia vietnam thailand filipina bangkok jepang buy one get dapat dua two satu meriah kirim send pengiriman paket hemat uang kembali dapat guarantee buatan lokal dalam internasional karya termurah paling murah terbaik cheap murah biaya\".split(' ')\n",
    "    return [w for w in list_text if w not in common_words]\n",
    "\n",
    "def remove_strange_words(list_text):\n",
    "    strange_words = ['aaa', 'aaaa', 'aaaaa', 'abc', 'abcd', 'bb', 'bbb', 'bbbb', 'ccc', 'cccc', 'thn', 'th', 'bln']\n",
    "    return [w for w in list_text if w not in strange_words]\n",
    "\n",
    "def string_escape(s, encoding='utf-8'):\n",
    "    return (\n",
    "        s.encode('latin1')  # To bytes, required by 'unicode-escape'\n",
    "        .decode('unicode-escape')  # Perform the actual octal-escaping decode\n",
    "        .encode('latin1')  # 1:1 mapping back to bytes\n",
    "        .decode(encoding)\n",
    "    )  # Decode original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "super-adelaide",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-09T20:37:18.778036Z",
     "iopub.status.busy": "2021-05-09T20:37:18.777122Z",
     "iopub.status.idle": "2021-05-09T20:37:18.779477Z",
     "shell.execute_reply": "2021-05-09T20:37:18.779887Z"
    },
    "papermill": {
     "duration": 0.013597,
     "end_time": "2021-05-09T20:37:18.780045",
     "exception": false,
     "start_time": "2021-05-09T20:37:18.766448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_vectorizer(max_features, max_len, vocab):\n",
    "    # max_features: Maximum vocab size.\n",
    "    # max_len: Sequence length to pad the outputs to.\n",
    "    \n",
    "    text_dataset = tf.data.Dataset.from_tensor_slices(vocab)\n",
    "    \n",
    "    # Create the layer.\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens = max_features,\n",
    "        output_mode = 'int',\n",
    "        output_sequence_length = max_len\n",
    "    )\n",
    "\n",
    "    vectorize_layer.adapt(text_dataset.batch(64))\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "    model.add(vectorize_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "precise-garden",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-09T20:37:18.795201Z",
     "iopub.status.busy": "2021-05-09T20:37:18.794252Z",
     "iopub.status.idle": "2021-05-09T20:37:24.638494Z",
     "shell.execute_reply": "2021-05-09T20:37:24.637378Z"
    },
    "papermill": {
     "duration": 5.852806,
     "end_time": "2021-05-09T20:37:24.638667",
     "exception": false,
     "start_time": "2021-05-09T20:37:18.785861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes: 11014\n"
     ]
    }
   ],
   "source": [
    "# preprocess df_unseen title & phash\n",
    "df['title'] = df['title'].apply(lambda x: string_escape(x))\n",
    "df['title'] = df['title'].apply(lambda x: remove_concatenate_2_words(x))\n",
    "df['title'] = df['title'].str.lower()\n",
    "df['title'] = df['title'].apply(lambda x: remove_punctuation(x))\n",
    "df['title'] = df['title'].apply(lambda x: str(x).split())\n",
    "df['title'] = df['title'].apply(lambda x: remove_stopwords(x))\n",
    "# df['title'] = df['title'].apply(lambda x: remove_big_number(x))\n",
    "df['title'] = df['title'].apply(lambda x: remove_zero_val(x))\n",
    "df['title'] = df['title'].apply(lambda x: remove_common_words(x))\n",
    "df['title'] = df['title'].apply(lambda x: remove_strange_words(x))\n",
    "df['title'] = df['title'].apply(lambda x: list(np.unique(x)))\n",
    "\n",
    "# title vocab\n",
    "words = list(df['title'])\n",
    "words = list(np.unique(np.concatenate(words)))\n",
    "\n",
    "# phash vocab\n",
    "phash = list(df['image_phash'].apply(lambda x: list(str(x))))\n",
    "phash = list(np.unique(np.concatenate(phash)))\n",
    "\n",
    "# Text vectorizer\n",
    "model = text_vectorizer(max_features = 25000, max_len = 100, vocab = words)\n",
    "list_text = [' '.join(x) for x in df['title']]\n",
    "title_vec = model.predict(list_text)\n",
    "df['title_vec'] = list(title_vec)\n",
    "\n",
    "model = text_vectorizer(max_features = 25, max_len = 25, vocab = phash)\n",
    "list_text = [' '.join(x) for x in df['image_phash']]\n",
    "phash_vec = model.predict(list_text)\n",
    "df['phash_vec'] = list(phash_vec)\n",
    "\n",
    "\n",
    "n_classes = df['label_group'].nunique()\n",
    "print(f'n_classes: {n_classes}')\n",
    "\n",
    "# save to file\n",
    "df.to_parquet(f'./train.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "european-palestine",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-09T20:37:24.655439Z",
     "iopub.status.busy": "2021-05-09T20:37:24.654784Z",
     "iopub.status.idle": "2021-05-09T20:37:24.908729Z",
     "shell.execute_reply": "2021-05-09T20:37:24.909177Z"
    },
    "papermill": {
     "duration": 0.264428,
     "end_time": "2021-05-09T20:37:24.909351",
     "exception": false,
     "start_time": "2021-05-09T20:37:24.644923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>title_vec</th>\n",
       "      <th>phash_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>[bag, paper, secret, victoria]</td>\n",
       "      <td>0</td>\n",
       "      <td>[22885, 8826, 5485, 1546, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[8, 13, 8, 10, 13, 2, 8, 14, 10, 4, 13, 5, 15,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>[12, 3m, 45, double, foam, mm, tape, vhb, x]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 18683, 17293, 10679, 3298, 1561, 878...</td>\n",
       "      <td>[7, 2, 14, 2, 8, 13, 11, 17, 5, 15, 9, 14, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>[397, canned, gr, luncheon, maling, meat, pork...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 20880, 16430, 11983, 11679, 11302, 7680, 2...</td>\n",
       "      <td>[6, 8, 13, 5, 6, 17, 17, 3, 4, 14, 3, 12, 17, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>[acak, alhadi, batik, campur, daster, dpt00100...</td>\n",
       "      <td>3</td>\n",
       "      <td>[24337, 23981, 22642, 20908, 19313, 18663, 140...</td>\n",
       "      <td>[9, 12, 16, 13, 2, 5, 12, 9, 3, 7, 2, 3, 7, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>[220ml, latte, nescafe, éclair]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 12631, 9898, 271, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7, 11, 2, 14, 16, 8, 2, 8, 15, 13, 7, 4, 10, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                     [bag, paper, secret, victoria]            0   \n",
       "1       [12, 3m, 45, double, foam, mm, tape, vhb, x]            1   \n",
       "2  [397, canned, gr, luncheon, maling, meat, pork...            2   \n",
       "3  [acak, alhadi, batik, campur, daster, dpt00100...            3   \n",
       "4                    [220ml, latte, nescafe, éclair]            4   \n",
       "\n",
       "                                           title_vec  \\\n",
       "0  [22885, 8826, 5485, 1546, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [1, 1, 1, 18683, 17293, 10679, 3298, 1561, 878...   \n",
       "2  [1, 20880, 16430, 11983, 11679, 11302, 7680, 2...   \n",
       "3  [24337, 23981, 22642, 20908, 19313, 18663, 140...   \n",
       "4  [1, 12631, 9898, 271, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           phash_vec  \n",
       "0  [8, 13, 8, 10, 13, 2, 8, 14, 10, 4, 13, 5, 15,...  \n",
       "1  [7, 2, 14, 2, 8, 13, 11, 17, 5, 15, 9, 14, 9, ...  \n",
       "2  [6, 8, 13, 5, 6, 17, 17, 3, 4, 14, 3, 12, 17, ...  \n",
       "3  [9, 12, 16, 13, 2, 5, 12, 9, 3, 7, 2, 3, 7, 15...  \n",
       "4  [7, 11, 2, 14, 16, 8, 2, 8, 15, 13, 7, 4, 10, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_parquet('./train.parquet', engine='pyarrow')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-duncan",
   "metadata": {
    "papermill": {
     "duration": 0.006252,
     "end_time": "2021-05-09T20:37:24.922019",
     "exception": false,
     "start_time": "2021-05-09T20:37:24.915767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.804426,
   "end_time": "2021-05-09T20:37:26.954903",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-09T20:37:04.150477",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
